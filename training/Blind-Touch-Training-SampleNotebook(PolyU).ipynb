{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Load the modules for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\" \n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow \n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from imgaug import augmenters as iaa\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Laoding the dataset\n",
    "\n",
    "- Load the preprocessed dataset\n",
    "- The size of the dataset is (R, L). In our setting, we define R, L = 224, 224\n",
    "- The first session contains 336 subjects with 6 fingerprint images\n",
    "- The first session contains 160 subjects with 6 fingerprint images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, L = 224, 224\n",
    "\n",
    "FIRST_SESSION_DATASET = 'Define the stored first session data path'\n",
    "SECOND_SESSION_DATASET = 'Define the stored first session data path'\n",
    "\n",
    "session1 = np.load(FIRST_SESSION_DATASET) \n",
    "session1 = session1.reshape([336, 6, R, L,1])\n",
    "\n",
    "session2 = np.load(SECOND_SESSION_DATASET)\n",
    "session2 = session2.reshape([160, 6, R, L,1])\n",
    "\n",
    "print(session1.shape, session2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construction of the data pairs.\n",
    "\n",
    "## 2-1. Setting the train set\n",
    "- Choose the first 136 subjects in the first session dataset. (It was drawn twice from the Genuine set to prevent the imbalance in the data ratio.)\n",
    "- Choose the first 160 subjects in the second session dataset.\n",
    "- DB_LIST_T_G contains pairs of genuine dataset and DB_LIST_T_I contains pairs of the imposter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "DB_LIST_T_G = []\n",
    "DB_LIST_T_I = []\n",
    "\n",
    "score = 0\n",
    "\n",
    "for i in range(0, 136):\n",
    "    for j in range(0, 5):\n",
    "        for k in range(j+1, 6):\n",
    "            photo1 = session1[i][j]/255.\n",
    "            photo2 = session1[i][k]/255.\n",
    "            DB_LIST_T_G.append([photo1, photo2, 1.0])\n",
    "\n",
    "for i in range(0, 136):\n",
    "    for j in range(0, 5):\n",
    "        for k in range(j+1, 6):\n",
    "            photo1 = session1[i][j]/255.\n",
    "            photo2 = session1[i][k]/255.\n",
    "            DB_LIST_T_G.append([photo2, photo1, 1.0])           \n",
    "    \n",
    "for i in range(0, 160):\n",
    "    for j in range(1, 5):\n",
    "        for k in range(j+1, 6):\n",
    "            photo1 = session2[i][j]/255.\n",
    "            photo2 = session2[i][k]/255.\n",
    "\n",
    "            DB_LIST_T_G.append([photo1, photo2, 1.0])\n",
    "            \n",
    "for i in range(0, 160):\n",
    "    for j in range(1, 5):\n",
    "        for k in range(j+1, 6):\n",
    "            photo1 = session2[i][j]/255.\n",
    "            photo2 = session2[i][k]/255.\n",
    "            DB_LIST_T_G.append([photo2, photo1, 1.0])\n",
    "            \n",
    "for i in range(0, 136):\n",
    "    for j in range(i+1, 137):\n",
    "        k1, k2 = random.randint(0,5), random.randint(0,5)\n",
    "        photo1 = session1[i][k1]/255.\n",
    "        photo2 = session1[j][k2]/255.\n",
    "        DB_LIST_T_I.append([photo1, photo2, 0.0])\n",
    "\n",
    "for i in range(0, 159):\n",
    "    for j in range(i+1, 160):\n",
    "        k1, k2 = random.randint(0,5), random.randint(0,5)\n",
    "        photo1 = session2[i][k1]/255.\n",
    "        photo2 = session2[j][k2]/255.\n",
    "        DB_LIST_T_I.append([photo1, photo2, 0.0])\n",
    "\n",
    "DB_LIST_T_G = np.array(DB_LIST_T_G)\n",
    "DB_LIST_T_I = np.array(DB_LIST_T_I)\n",
    "np.random.shuffle(DB_LIST_T_G) \n",
    "np.random.shuffle(DB_LIST_T_I) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Setting the test set\n",
    "- Choose the last 200 subjects in the first session dataset. \n",
    "- DB_LIST_TEST contains pairs of test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_LIST_TEST = []\n",
    "\n",
    "score = 0\n",
    "for i in range(136, 336):\n",
    "    for j in range(0, 5):\n",
    "        for k in range(j+1, 6):\n",
    "            photo1 = session1[i][j]/255.\n",
    "            photo2 = session1[i][k]/255.\n",
    "            DB_LIST_TEST.append([photo1, photo2, 1.0])\n",
    "\n",
    "for i in range(136, 335):\n",
    "    for j in range(i+1, 336):\n",
    "        k1, k2 = random.randint(0,5), random.randint(0,5)\n",
    "        photo1 = session1[i][k1]/255.\n",
    "        photo2 = session1[j][k2]/255.\n",
    "        DB_LIST_TEST.append([photo1, photo2, 0.0])\n",
    "        \n",
    "DB_LIST_TEST = np.array(DB_LIST_TEST)                \n",
    "np.random.shuffle(DB_LIST_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting Parameters\n",
    "\n",
    "- For augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = (R, L, 1) \n",
    "EPOCHS = 150\n",
    "\n",
    "train_seq = iaa.Sequential([\n",
    "    iaa.GaussianBlur(sigma=(0, 0.3)),\n",
    "    iaa.Dropout((0.01, 0.15), per_channel=0.5),\n",
    "    iaa.Affine(\n",
    "        translate_percent={\"x\": (-0.15, 0.2), \"y\": (-0.15, 0.2)},\n",
    "        order=[0, 1],\n",
    "        cval=1\n",
    "    )\n",
    "], random_order=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define the Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, x1, x2, batch_size=32, shuffle=True):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.x1)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1_batch, x2_batch, y_batch = [], [], []\n",
    "    \n",
    "        x1_list = self.x1[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        x2_list = self.x2[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "                \n",
    "        for i in range(self.batch_size):\n",
    "            if random.random() > 0.5:\n",
    "                a = x1_list[i]\n",
    "            else:\n",
    "                a = x2_list[i]\n",
    "            \n",
    "            x1_batch.append(a[0])\n",
    "            x2_batch.append(a[1])\n",
    "            y_batch.append(a[2])\n",
    "             \n",
    "            \n",
    "            \n",
    "        x1_batch, x2_batch = np.array(x1_batch).astype(np.float32), np.array(x2_batch).astype(np.float32)\n",
    "        \n",
    "        x1_batch = train_seq.augment_images(x1_batch)\n",
    "        x2_batch = train_seq.augment_images(x2_batch)\n",
    "        return [x1_batch.astype(np.float32) / 1., x2_batch.astype(np.float32) / 1.], np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            self.x1 = shuffle(self.x1) \n",
    "            self.x2 = shuffle(self.x2) \n",
    "            \n",
    "            \n",
    "class TestDataGenerator(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, x, batch_size=32, shuffle=True):\n",
    "        self.x = x\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1_batch, x2_batch, y_batch = [], [], []\n",
    "        \n",
    "        x_list = self.x[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        for a in x_list:\n",
    "            x1_batch.append(a[0])\n",
    "            x2_batch.append(a[1])\n",
    "            y_batch.append(a[2])\n",
    "            \n",
    "        x1_batch, x2_batch = np.array(x1_batch), np.array(x2_batch)\n",
    "        \n",
    "        return [x1_batch.astype(np.float32) / 1., x2_batch.astype(np.float32) / 1.], np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            self.x = shuffle(self.x)\n",
    "    \n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "swish = tensorflow.keras.activations.swish\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "train_gen = TrainDataGenerator(DB_LIST_T_G, DB_LIST_T_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define the model\n",
    "\n",
    "- Define the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1 = layers.Input(shape=(R, L, 1))\n",
    "x2 = layers.Input(shape=(R, L, 1))\n",
    "\n",
    "inputs = layers.Input(shape=(R, L, 1))\n",
    "\n",
    "padding = 'same'\n",
    "activation = swish\n",
    "pool_size = 2\n",
    "\n",
    "feature = layers.Conv2D(32, kernel_size=3, padding=padding, strides=1)(inputs)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(64, kernel_size=3, padding=padding, strides=1)(feature)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(128, kernel_size=3, padding=padding, strides=1)(feature)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(256, kernel_size=3, padding=padding, strides=1)(feature)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(512, kernel_size=3, padding=padding, strides=1)(feature) \n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature_model = Model(inputs=inputs, outputs=feature)\n",
    "\n",
    "x1_net = feature_model(x1)\n",
    "x2_net = feature_model(x2)\n",
    "\n",
    "x1_net = layers.Flatten()(x1_net)\n",
    "x2_net = layers.Flatten()(x2_net)\n",
    "x1_net = keras.layers.UnitNormalization()(x1_net)\n",
    "x2_net = keras.layers.UnitNormalization()(x2_net)\n",
    "\n",
    "net = layers.Subtract()([x1_net, x2_net])\n",
    "net = layers.Dense(16)(net)\n",
    "net = square(net)\n",
    "net = layers.Dense(1, activation='sigmoid')(net)\n",
    "\n",
    "model = Model(inputs=[x1, x2], outputs=net)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model\n",
    "\n",
    "- Train the defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, epochs=150, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save the model\n",
    "- Save the weights of the trained feature model and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_MODEL_PATH = 'Define the path to store the weigths of the feature model'\n",
    "MODEL_PATH = 'Define the path to store the weigths of the model'\n",
    "\n",
    "feature_model.save(FEATURE_MODEL_PATH)\n",
    "model.save(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "80e3b7918d87f9b73f4273d3d95ca7aa4d0a4a599e820d7ca969efdb3d0e945e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
