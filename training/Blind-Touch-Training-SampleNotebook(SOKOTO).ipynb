{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "Load the modules for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\" \n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow \n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imgaug import augmenters as iaa\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Laoding the dataset\n",
    "\n",
    "- Load the preprocessed dataset\n",
    "- The size of the dataset is (R, L). In our setting, we define R, L = 224, 224\n",
    "- The first session contains 336 subjects with 6 fingerprint images\n",
    "- The first session contains 160 subjects with 6 fingerprint images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, L = 224, 224\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = (R, L, 1)\n",
    "\n",
    "train_seq = iaa.Sequential([\n",
    "    iaa.GaussianBlur(sigma=(0, 0.7)),\n",
    "    iaa.Dropout((0.01, 0.15), per_channel=0.5),\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "        rotate=(-30, 30),\n",
    "        order=[0, 1],\n",
    "        cval=1\n",
    "    )\n",
    "], random_order=True)\n",
    "\n",
    "test_seq = iaa.Sequential([\n",
    "    iaa.GaussianBlur(sigma=(0, 0.7)),\n",
    "    iaa.Dropout((0.01, 0.15), per_channel=0.5),\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "        rotate=(-30, 30),\n",
    "        order=[0, 1],\n",
    "        cval=1\n",
    "    )\n",
    "], random_order=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the Dataset\n",
    "- Load the data and split dataset with training, validation, and test.\n",
    "- Change the type as float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SOKOTO_DATA_PATH = 'Define the stored data path'\n",
    "DATA_SIZE = 6000\n",
    "x_real = np.load(SOKOTO_DATA_PATH)  / 255.\n",
    "y_real = np.zeros(DATA_SIZE, dtype=int)\n",
    "for i in range(DATA_SIZE):\n",
    "    y_real[i] = i\n",
    "    \n",
    "x_train, x_test, label_train, label_test = train_test_split(x_real, y_real, test_size=0.2, random_state = 42)\n",
    "x_train, x_val, label_train, label_val = train_test_split(x_train, label_train, test_size=0.25, random_state = 42)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, x, label, x_real, y_real, batch_size=4, shuffle=True):\n",
    "        self.x = x\n",
    "        self.label = label\n",
    "        self.x_real = x_real\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1_batch = self.x[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        label_batch = self.label[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        x2_batch = np.empty((self.batch_size, R, L, 1), dtype=np.float32)\n",
    "        y_batch = np.zeros((self.batch_size, 1), dtype=np.float32)\n",
    "        \n",
    "        x1_batch, x2_batch = np.array(x1_batch), np.array(x2_batch)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            x1_batch = train_seq.augment_images(x1_batch)\n",
    "        \n",
    "        for i, idx in enumerate(label_batch):\n",
    "            if random.random() > 0.5:\n",
    "                x2_batch[i] = self.x_real[idx]\n",
    "                y_batch[i] = 1.\n",
    "            else:\n",
    "                not_chosen_idx = random.choice(list(set(y_real) - set([idx])))\n",
    "                x2_batch[i] = self.x_real[not_chosen_idx]\n",
    "                y_batch[i] = 0.\n",
    "\n",
    "        return [x1_batch.astype(np.float32) / 1., x2_batch.astype(np.float32) / 1.], y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            self.x, self.label = shuffle(self.x, self.label)\n",
    "            \n",
    "class ValDataGenerator(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, x, label, x_real, y_real, batch_size=4, shuffle=True):\n",
    "        self.x = x\n",
    "        self.label = label\n",
    "        self.x_real = x_real\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1_batch = self.x[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        label_batch = self.label[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        x2_batch = np.empty((self.batch_size, R, L, 1), dtype=np.float32)\n",
    "        y_batch = np.zeros((self.batch_size, 1), dtype=np.float32)\n",
    "        \n",
    "        x1_batch, x2_batch = np.array(x1_batch), np.array(x2_batch)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            x1_batch = train_seq.augment_images(x1_batch)\n",
    "        \n",
    "        for i, idx in enumerate(label_batch):\n",
    "            if random.random() > 0.5:\n",
    "                x2_batch[i] = self.x_real[idx]\n",
    "                y_batch[i] = 1.\n",
    "            else:\n",
    "                not_chosen_idx = random.choice(list(set(y_real) - set([idx])))\n",
    "                x2_batch[i] = self.x_real[not_chosen_idx]\n",
    "                y_batch[i] = 0.\n",
    "\n",
    "        return [x1_batch.astype(np.float32) / 1., x2_batch.astype(np.float32) / 1.], y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            self.x, self.label = shuffle(self.x, self.label)\n",
    "            \n",
    "train_gen = DataGenerator(x_train, label_train, x_real, y_real, shuffle=True)\n",
    "val_gen = ValDataGenerator(x_val, label_val, x_real, y_real, shuffle=True)\n",
    "test_gen = ValDataGenerator(x_test, label_test, x_real, y_real, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "swish = tensorflow.keras.activations.swish\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = layers.Input(shape=(R, L, 1))\n",
    "x2 = layers.Input(shape=(R, L, 1))\n",
    "\n",
    "inputs = layers.Input(shape=(R, L, 1))\n",
    "\n",
    "padding = 'same'\n",
    "activation = swish\n",
    "pool_size = 2\n",
    "\n",
    "feature = layers.Conv2D(32, kernel_size=3, padding=padding, strides=1)(inputs)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(64, kernel_size=3, padding=padding, strides=1)(feature)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(128, kernel_size=3, padding=padding, strides=1)(feature)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(256, kernel_size=3, padding=padding, strides=1)(feature)\n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature = layers.Conv2D(512, kernel_size=3, padding=padding, strides=1)(feature) \n",
    "feature = layers.BatchNormalization()(feature) \n",
    "feature = activation(feature)\n",
    "layers.Dropout(.4, input_shape=(2,))\n",
    "feature = layers.MaxPooling2D(pool_size=pool_size)(feature)\n",
    "\n",
    "feature_model = Model(inputs=inputs, outputs=feature)\n",
    "\n",
    "x1_net = feature_model(x1)\n",
    "x2_net = feature_model(x2)\n",
    "\n",
    "x1_net = layers.Flatten()(x1_net)\n",
    "x2_net = layers.Flatten()(x2_net)\n",
    "x1_net = keras.layers.UnitNormalization()(x1_net)\n",
    "x2_net = keras.layers.UnitNormalization()(x2_net)\n",
    "net = layers.Subtract()([x1_net, x2_net])\n",
    "\n",
    "net = layers.Dense(16)(net)\n",
    "net = square(net)\n",
    "net = layers.Dense(1, activation='sigmoid')(net)\n",
    "\n",
    "model = Model(inputs=[x1, x2], outputs=net)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model\n",
    "\n",
    "- Train the defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, epochs=150, validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save the model\n",
    "- Save the weights of the trained feature model and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_MODEL_PATH = 'Define the path to store the weigths of the feature model'\n",
    "MODEL_PATH = 'Define the path to store the weigths of the model'\n",
    "\n",
    "feature_model.save(FEATURE_MODEL_PATH)\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET_PATH = 'Define the path to store the testset'\n",
    "np.save(TESTSET_PATH, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "80e3b7918d87f9b73f4273d3d95ca7aa4d0a4a599e820d7ca969efdb3d0e945e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
